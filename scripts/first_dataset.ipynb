{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/haunted_places.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 205\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# Display the first few rows to verify if needed\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m#print(haunted_gdf.head())\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m haunted_gdf\n\u001b[0;32m--> 205\u001b[0m new1234\u001b[38;5;241m=\u001b[39madd_public_school_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/haunted_places.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m, in \u001b[0;36madd_public_school_dataset\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mThis function takes tsv file input, joins with public school dataset by adding 3 new features to initial input\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#load haunted places dataset from tsv\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#file_path = \"data/daylight_added.tsv\"\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, on_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Load the Public Schools GeoJSON File\u001b[39;00m\n\u001b[1;32m     28\u001b[0m schools_gdf \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mread_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/Public_Schools_-5088709809754466635.geojson\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/haunted_places.tsv'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_public_school_dataset(file_path):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes tsv file input, joins with public school dataset by adding 3 new features to initial input\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #load haunted places dataset from tsv\n",
    "    #file_path = \"data/daylight_added.tsv\"\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", on_bad_lines=\"skip\")\n",
    "\n",
    "    # Load the Public Schools GeoJSON File\n",
    "    schools_gdf = gpd.read_file(\"data/Public_Schools_-5088709809754466635.geojson\")\n",
    "\n",
    "    #print(gdf.head())\n",
    "    #print(gdf.info())\n",
    "    #print(gdf.describe())\n",
    "\n",
    "    # Basic plot of the geometries to make sure it has high coverage\n",
    "    schools_gdf.plot(figsize=(10, 6), edgecolor=\"black\", cmap=\"viridis\")\n",
    "    plt.title(\"GeoJSON Spatial Data\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert Public Schools GeoJSON to CSV (including lat-long).\n",
    "\n",
    "    # Since GeoJSON contains geometries, we need to extract latitude and longitude before saving it to CSV.\n",
    "\n",
    "    # Extract latitude & longitude\n",
    "    gdf[\"latitude\"] = gdf.geometry.y\n",
    "    gdf[\"longitude\"] = gdf.geometry.x\n",
    "\n",
    "    # Convert to DataFrame (removing geometry)\n",
    "    schools_df = gdf.drop(columns=[\"geometry\"])\n",
    "\n",
    "    # Save as CSV\n",
    "    schools_df.to_csv(\"public_schools.csv\", index=False)\n",
    "\n",
    "    print(\"Public Schools dataset successfully converted to CSV.\")\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Convert Haunted Places to GeoDataFrame type for analysis\n",
    "    haunted_gdf = gpd.GeoDataFrame(\n",
    "    df, \n",
    "    geometry=[Point(lon, lat) for lon, lat in zip(df[\"longitude\"], df[\"latitude\"])], \n",
    "    crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert Schools CSV to GeoDataFrame\n",
    "    schools_gdf = gpd.GeoDataFrame(\n",
    "    schools_df, \n",
    "    geometry=[Point(lon, lat) for lon, lat in zip(schools_df[\"longitude\"], schools_df[\"latitude\"])], \n",
    "    crs=\"EPSG:4326\"\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Display structure\n",
    "    #print(schools_gdf.head())\n",
    "    #print(haunted_gdf.head())\n",
    "\n",
    "    #adding feature 1 -> number of schools within 10 miles of each haunted place\n",
    "\n",
    "    #Converting to Coordinate Reference System (CRS) for Distance Calculation We need to convert both datasets into a projected coordinate system (EPSG:3857) for accurate distance computations.\n",
    "\n",
    "\n",
    "    # Convert both datasets to EPSG:3857 (meters-based projection)\n",
    "    schools_gdf = schools_gdf.to_crs(epsg=3857)  \n",
    "    haunted_gdf = haunted_gdf.to_crs(epsg=3857)\n",
    "\n",
    "    #Creating a Buffer Around Each Haunted Place We'll create a 10-mile buffer around each haunted place (1 mile ≈ 1609.34 meters).\n",
    "\n",
    "\n",
    "    haunted_gdf[\"buffer_10_miles\"] = haunted_gdf.geometry.buffer(10 * 1609.34)\n",
    "\n",
    "\n",
    "    # Function to count schools within the buffer of each haunted place\n",
    "    def count_schools_within_buffer(haunted_row):\n",
    "        return schools_gdf[schools_gdf.geometry.within(haunted_row[\"buffer_10_miles\"])].shape[0]\n",
    "\n",
    "    # Apply function to count schools near each haunted place\n",
    "    haunted_gdf[\"schools_within_10_miles\"] = haunted_gdf.apply(count_schools_within_buffer, axis=1)\n",
    "\n",
    "    #haunted_gdf[\"schools_within_10_miles\"].describe()\n",
    "\n",
    "\n",
    "    #visualizing feature 1 (might be used for step 6) ___REMOVE IF NEEDED___\n",
    "\n",
    "    \"\"\"\n",
    "    # Column to visualize\n",
    "    column_name = \"schools_within_10_miles\"  # Change this to any column in your df\n",
    "\n",
    "    # Plot the distribution\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(haunted_gdf[column_name], bins=20, kde=True, color=\"blue\")\n",
    "\n",
    "    # Labels & Title\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Distribution of {column_name}\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    # Adding feature 2 \"distance to nearest school\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_nearest_distances(haunted_gdf, schools_gdf, method=\"haversine\"):\n",
    "        \"\"\"\n",
    "        Computes the nearest school distance for each haunted location using BallTree.\n",
    "\n",
    "        Parameters:\n",
    "            haunted_gdf (DataFrame): DataFrame containing haunted locations with 'latitude' and 'longitude'.\n",
    "            schools_df (DataFrame): DataFrame containing school locations with 'latitude' and 'longitude'.\n",
    "            method (str): \"haversine\" (fastest) or \"euclidean\" (for planar distances).\n",
    "\n",
    "        Returns:\n",
    "            haunted_gdf with an additional column 'distance_to_nearest_school_km'.\n",
    "        \"\"\"\n",
    "        # Drop rows with NaN values in latitude/longitude\n",
    "        haunted_gdf = haunted_gdf.dropna(subset=[\"latitude\", \"longitude\"]).reset_index(drop=True)\n",
    "        schools_gdf = schools_gdf.dropna(subset=[\"latitude\", \"longitude\"]).reset_index(drop=True)\n",
    "\n",
    "        # Convert degrees to radians for BallTree (needed for haversine distance)\n",
    "        haunted_coords = np.radians(haunted_gdf[[\"latitude\", \"longitude\"]].to_numpy())\n",
    "        school_coords = np.radians(schools_gdf[[\"latitude\", \"longitude\"]].to_numpy())\n",
    "\n",
    "        # Check if we have at least one school location\n",
    "        if school_coords.shape[0] == 0:\n",
    "            print(\"⚠ No valid school coordinates found. Setting distance to NaN.\")\n",
    "            haunted_gdf[\"distance_to_nearest_school_km\"] = np.nan\n",
    "            return haunted_gdf\n",
    "\n",
    "        # Build BallTree with school locations\n",
    "        tree = BallTree(school_coords, metric=\"haversine\")\n",
    "\n",
    "        # Query the nearest school for each haunted location\n",
    "        distances, _ = tree.query(haunted_coords, k=1)  # k=1 means nearest neighbor\n",
    "\n",
    "        # Convert from radians to kilometers (Earth's radius = 6371 km)\n",
    "        haunted_gdf[\"distance_to_nearest_school_km\"] = distances[:, 0] * 6371\n",
    "\n",
    "        return haunted_gdf\n",
    "\n",
    "    # Choose method (Haversine is best for geographic distance)\n",
    "    distance_method = \"haversine\"\n",
    "\n",
    "    # Compute distances using BallTree\n",
    "    haunted_gdf = compute_nearest_distances(haunted_gdf, schools_gdf, method=distance_method)\n",
    "\n",
    "    # Save the dataset after feature 2 if needed\n",
    "    #haunted_gdf.to_csv(\"haunted_places_with_distance.csv\", index=False)\n",
    "\n",
    "    #adding feature 3 \"is_haunted_place_a_school?\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Create a new column 'is_haunted_place_school?' based on whether 'school' appears in 'location'\n",
    "    haunted_gdf[\"is_haunted_place_a_school?\"] = haunted_gdf[\"location\"].str.contains(\"school\", case=False, na=False).map({True: \"Yes\", False: \"No\"})\n",
    "\n",
    "    # Save the updated dataset\n",
    "    haunted_gdf.to_csv(\"haunted_places_with_new_features.csv\", index=False)\n",
    "\n",
    "    # Display the first few rows to verify if needed\n",
    "    #print(haunted_gdf.head())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return haunted_gdf\n",
    "\n",
    "new1234=add_public_school_dataset(\"data/haunted_places.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
